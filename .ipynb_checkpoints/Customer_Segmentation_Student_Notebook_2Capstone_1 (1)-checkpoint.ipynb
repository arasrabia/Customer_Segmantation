{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG6Ti6UOiqav"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2v-2JDuiqa0"
   },
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLS5y2Jqiqa1"
   },
   "source": [
    "Welcome to **\"RFM Customer Segmentation & Cohort Analysis Project\"**. This is the first project of the Capstone Project Series, which consists of 3 different project that contain different scenarios.\n",
    "\n",
    "This is a project which you will learn what is RFM? And how to apply **RFM Analysis** and Customer Segmentation using **K-Means Clustering**. Also you will improve your Data Cleaning, Data Visualization and Exploratory Data Analysis capabilities. On the other hand you will create Cohort and Conduct **Cohort Analysis**. \n",
    "\n",
    "Before diving into the project, please take a look at the determines and project structure.\n",
    "\n",
    "- **NOTE:** This Project assumes that you already know the basics of coding in Python and are familiar with the theory behind K-Means Clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SCOFEfqiqa1"
   },
   "source": [
    "# #Determines\n",
    "\n",
    "Using the [Online Retail dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail) from the UCI Machine Learning Repository for exploratory data analysis, ***Customer Segmentation***, ***RFM Analysis***, ***K-Means Clustering*** and ***Cohort Analysis***.\n",
    "\n",
    "This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "Feature Information:\n",
    "\n",
    "**InvoiceNo**: Invoice number. *Nominal*, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n",
    "<br>\n",
    "**StockCode**: Product (item) code. *Nominal*, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "<br>\n",
    "**Description**: Product (item) name. *Nominal*. \n",
    "<br>\n",
    "**Quantity**: The quantities of each product (item) per transaction. *Numeric*.\n",
    "<br>\n",
    "**InvoiceDate**: Invoice Date and time. *Numeric*, the day and time when each transaction was generated.\n",
    "<br>\n",
    "**UnitPrice**: Unit price. *Numeric*, Product price per unit in sterling.\n",
    "<br>\n",
    "**CustomerID**: Customer number. *Nominal*, a 5-digit integral number uniquely assigned to each customer.\n",
    "<br>\n",
    "**Country**: Country name. *Nominal*, the name of the country where each customer resides.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "First of all, to observe the structure of the data and missing values, you can use exploratory data analysis and data visualization techniques.\n",
    "\n",
    "You must do descriptive analysis. Because you must understand the relationship of the features to each other and clear the noise and missing values in the data. After that, the data set will be ready for RFM analysis.\n",
    "\n",
    "Before starting the RFM Analysis, you will be asked to do some analysis regarding the distribution of *Orders*, *Customers* and *Countries*. These analyzes will help the company develop its sales policies and contribute to the correct use of resources.\n",
    "\n",
    "You will notice that the UK not only has the most sales revenue, but also the most customers. So you will continue to analyze only UK transactions in the next RFM Analysis, Customer Segmentation and K-Means Clustering topics.\n",
    "\n",
    "Next, you will begin RFM Analysis, a customer segmentation technique based on customers' past purchasing behavior. \n",
    "\n",
    "By using RFM Analysis, you can enable companies to develop different approaches to different customer segments so that they can get to know their customers better, observe trends better, and increase customer retention and sales revenues.\n",
    "\n",
    "You will calculate the Recency, Frequency and Monetary values of the customers in the RFM Analysis you will make using the data consisting of UK transactions. Ultimately, you have to create an RFM table containing these values.\n",
    "\n",
    "In the Customer Segmentation section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\".\n",
    "\n",
    "We will segment the customers ourselves based on their recency, frequency, and monetary values. But can an **unsupervised learning** model do this better for us? You will use the K-Means algorithm to find the answer to this question. Then you will compare the classification made by the algorithm with the classification you have made yourself.\n",
    "\n",
    "Before applying K-Means Clustering, you should do data pre-processing. In this context, it will be useful to examine feature correlations and distributions. In addition, the data you apply for K-Means should be normalized.\n",
    "\n",
    "On the other hand, you should inform the K-means algorithm about the number of clusters it will predict. You will also try the *** Elbow method *** and *** Silhouette Analysis *** to find the optimum number of clusters.\n",
    "\n",
    "After the above operations, you will have made cluster estimation with K-Means. You should visualize the cluster distribution by using a scatter plot. You can observe the properties of the resulting clusters with the help of the boxplot. Thus you will be able to tag clusters and interpret results.\n",
    "\n",
    "Finally, you will do Cohort Analysis with the data you used at the beginning, regardless of the analysis you have done before. Cohort analysis is a subset of behavioral analytics that takes the user data and breaks them into related groups for analysis. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ62QseViqa2"
   },
   "source": [
    "# #Project Structures\n",
    "\n",
    "- Data Cleaning & Exploratory Data Analysis\n",
    "- RFM Analysis\n",
    "- Customer Segmentation\n",
    "- Applying K-Means Clustering\n",
    "- Create Cohort and Conduct Cohort Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsPQ1tUwiqa2"
   },
   "source": [
    "# #Tasks\n",
    "\n",
    "#### 1. Data Cleaning & Exploratory Data Analysis\n",
    "\n",
    "- Import Modules, Load Data & Data Review\n",
    "- Follow the Steps Below\n",
    "\n",
    "    *i. Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns.*\n",
    "    \n",
    "    *ii. What does the letter \"C\" in the invoiceno column mean?*\n",
    "    \n",
    "    *iii. Handling Missing Values*\n",
    "    \n",
    "    *iv. Clean the Data from the Noise and Missing Values*\n",
    "    \n",
    "    *v. Explore the Orders*\n",
    "    \n",
    "    *vi. Explore Customers by Country*\n",
    "    \n",
    "    *vii. Explore the UK Market*\n",
    "    \n",
    "#### 2. RFM Analysis\n",
    "\n",
    "- Follow the steps below\n",
    "\n",
    "   *i. Import Libraries*\n",
    "   \n",
    "   *ii. Review \"df_uk\" DataFrame (the df_uk what you create at the end of the Task 1)*\n",
    "   \n",
    "   *iii. Calculate Recency*\n",
    "   \n",
    "   *iv. Calculate Frequency*\n",
    "   \n",
    "   *v. Calculate Monetary Values*\n",
    "   \n",
    "   *vi. Create RFM Table*\n",
    "\n",
    "#### 3. Customer Segmentation with RFM Scores\n",
    "- Calculate RFM Scoring\n",
    "\n",
    "    *i. Creating the RFM Segmentation Table*\n",
    " \n",
    "- Plot RFM Segments\n",
    "\n",
    "#### 4. Applying K-Means Clustering\n",
    "- Data Pre-Processing and Exploring\n",
    "\n",
    "    *i. Define and Plot Feature Correlations*\n",
    " \n",
    "    *ii. Visualize Feature Distributions*\n",
    " \n",
    "    *iii. Data Normalization*\n",
    "\n",
    "- K-Means Implementation\n",
    "\n",
    "    *i. Define Optimal Cluster Number (K) by using \"Elbow Method\" and \"Silhouette Analysis\"*\n",
    " \n",
    "    *ii. Visualize the Clusters*\n",
    " \n",
    "    *iii. Assign the label*\n",
    " \n",
    "    *iv. Conclusion*\n",
    " \n",
    "#### 5. Create Cohort and Conduct Cohort Analysis\n",
    "- Future Engineering\n",
    "\n",
    "    *i. Extract the Month of the Purchase*\n",
    " \n",
    "    *ii. Calculating time offset in Months i.e. Cohort Index*\n",
    " \n",
    "- Create 1st Cohort: User Number & Retention Rate \n",
    "\n",
    "    *i. Pivot Cohort and Cohort Retention*\n",
    " \n",
    "    *ii. Visualize analysis of cohort 1 using seaborn and matplotlib*\n",
    "\n",
    "- Create 2nd Cohort: Average Quantity Sold \n",
    "\n",
    "    *i. Pivot Cohort and Cohort Retention*\n",
    " \n",
    "    *ii. Visualize analysis of cohort 2 using seaborn and matplotlib*\n",
    "\n",
    "- Create 3rd Cohort: Average Sales\n",
    "\n",
    "    *i. Pivot Cohort and Cohort Retention*\n",
    " \n",
    "    *ii. Visualize analysis of cohort 3 using seaborn and matplotlib*\n",
    "    \n",
    "- **Note: There may be sub-tasks associated with each task, you will see them in order during the course of the work.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-NlVU1UQGVA"
   },
   "source": [
    "# 1. Data Cleaning & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L63G_-Dqiqa3"
   },
   "source": [
    "## Import Modules, Load Data & Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skimpy in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (0.0.5)\n",
      "Collecting click==7.1.2\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.12.1 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from skimpy) (2.13.3)\n",
      "Requirement already satisfied: rich<11.0.0,>=10.9.0 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from skimpy) (10.16.2)\n",
      "Requirement already satisfied: Pygments<3.0.0,>=2.10.0 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from skimpy) (2.11.2)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.2 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from skimpy) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0.0,>=1.3.2->skimpy) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0.0,>=1.3.2->skimpy) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from pandas<2.0.0,>=1.3.2->skimpy) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.3.2->skimpy) (1.16.0)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from rich<11.0.0,>=10.9.0->skimpy) (0.4.4)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /Users/rabiaaras/opt/anaconda3/lib/python3.9/site-packages (from rich<11.0.0,>=10.9.0->skimpy) (0.9.1)\n",
      "Installing collected packages: click\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Uninstalling click-8.0.4:\n",
      "      Successfully uninstalled click-8.0.4\n",
      "Successfully installed click-7.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install skimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J-Zb5JfOiqa3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from skimpy import clean_columns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/gozdesim/Downloads/Claurusway/Data Science/Capstone Project/1. RFM Customer Segmentation & Cohort Analysis ProjectPage/Online Retail.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/gozdesim/Downloads/Claurusway/Data Science/Capstone Project/1. RFM Customer Segmentation & Cohort Analysis ProjectPage/Online Retail.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/gozdesim/Downloads/Claurusway/Data Science/Capstone Project/1. RFM Customer Segmentation & Cohort Analysis ProjectPage/Online Retail.xlsx'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(r'/Users/gozdesim/Downloads/Claurusway/Data Science/Capstone Project/1. RFM Customer Segmentation & Cohort Analysis ProjectPage/Online Retail.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()*100/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['CustomerID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['InvoiceNo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YECMxCzUQGV7"
   },
   "source": [
    "### i. Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECXSVJAucLVE"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['InvoiceNo', 'Quantity', 'UnitPrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['InvoiceNo', 'Quantity', 'UnitPrice']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiSW67N_QGV8"
   },
   "source": [
    "We see that there are negative values in the Quantity and UnitPrice columns. These are possibly canceled and returned orders. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts of the negative values in the Quantity column\n",
    "df[df['Quantity']<0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Quantity']<0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts of the negative values in the UnitPrice column\n",
    "df[df['UnitPrice']<0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['UnitPrice']<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OoPE-QLiqa4"
   },
   "source": [
    "### ii. What does the letter \"C\" in the InvoiceNo column mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt32SZgJQGV8"
   },
   "source": [
    "If the invoice number starts with the letter \"C\", it means the order was cancelled. Or those who abandon their order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP5RPZq1QGV_"
   },
   "source": [
    "When we filter canceled orders by Quantity> 0 or filter non-canceled orders by Quantity <0 nothing returns, this confirms that negative values mean the order was canceled. So lets find out how many orders were cancelled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['canceled']=df.InvoiceNo.str.extract('([C])').fillna(0).replace({'C':1})\n",
    "df['canceled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.canceled==1) & (df.Quantity>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrp8logRQGWA"
   },
   "source": [
    "#### 3836 or about 15% of the orders were cancelled. Looking deeper into why these orders were cancelled may prevent future cancellations. Now let's find out what a negative UnitPrice means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['canceled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.canceled==1]['CustomerID'].nunique()/df['CustomerID'].nunique()*100\n",
    "#we are getting the proportion of the customers who canceled their order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are getting the proportion of the canceled order\n",
    "df[df.canceled==1]['InvoiceNo'].nunique()/df[['InvoiceNo']].nunique()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.CustomerID==14397].tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXVVls6sQGVQ"
   },
   "source": [
    "### iii. Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQtZK5paQGVf"
   },
   "source": [
    "Since the customer ID's are missing, lets assume these orders were not made by the customers already in the data set because those customers already have ID's. \n",
    "\n",
    "We also don't want to assign these orders to those customers because this would alter the insights we draw from the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.canceled==1]['CustomerID'].value_counts(dropna=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llu-bMTAiqa6"
   },
   "source": [
    "### iv. Clean the Data from the Noise and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.CustomerID.notnull()] #dropping CustomerID's with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.Quantity>0)&(df.UnitPrice>0)] #dropping Quantity and UnitPrice with any negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.canceled==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('canceled', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25MkNjZqQGWC"
   },
   "source": [
    "### v. Explore the Orders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OAkPoUjiqa7"
   },
   "source": [
    "1. Find the unique number of InvoiceNo  per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3tLAfLacgqY"
   },
   "outputs": [],
   "source": [
    "df.groupby('CustomerID')['InvoiceNo'].nunique().sort_values(ascending=False)\n",
    "#getting the total number of orders per customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di03OKjzQGWE"
   },
   "source": [
    "2. What's the average number of unqiue items per order or per customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxkeKq08ciqJ"
   },
   "outputs": [],
   "source": [
    "mean_of_unique_items= round(df.groupby(['CustomerID',\n",
    "                      'InvoiceNo']).agg({'StockCode':lambda x:x.nunique()}).groupby('CustomerID')['StockCode'].mean(),\n",
    "                      1).sort_values(ascending=False)\n",
    "mean_of_unique_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUKzA73dQGWH"
   },
   "source": [
    "3. Let's see how this compares to the number of unique products per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niQq9uyuck5z"
   },
   "outputs": [],
   "source": [
    "num_of_unique_product = pd.DataFrame(df.groupby('CustomerID').StockCode.nunique()).rename(columns={'StockCode': 'num_of_unique_product'})\n",
    "\n",
    "num_of_order = df.groupby('CustomerID').InvoiceNo.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([mean_of_unique_items,\n",
    "          num_of_order,\n",
    "          num_of_unique_product],\n",
    "         axis=1).rename(columns={'StockCode': 'mean_of_unique_items',\n",
    "                                'InvoiceNo': 'num_of_order'}).sort_values('num_of_order', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Ozp-U5QGWK"
   },
   "source": [
    "### vi. Explore Customers by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP6M3isLiqa8"
   },
   "source": [
    "1. What's the total revenue per country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OcfXfELcmmY"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalPrice'] = df['UnitPrice']*df['Quantity']\n",
    "df                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(df.groupby('Country').TotalPrice.sum().apply(lambda x: round(x,2))).sort_values('TotalPrice', ascending=False)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['perc_of_TotalPrice']=round(df2.TotalPrice/df2.TotalPrice.sum()*100,2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk64qtEliqa8"
   },
   "source": [
    "2. Visualize number of customer per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTBwz6Z6comb"
   },
   "outputs": [],
   "source": [
    "df2['customer_num']=df.groupby('Country').CustomerID.nunique()\n",
    "df2['customer_rate']=round(df2.customer_num/df2.customer_num.sum()*100,2)\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(y=df2.index, x=df2.customer_num.sort_values(ascending=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w/out United Kingdom\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(y=df2.iloc[1:].index, x=df2.iloc[1:].customer_num.sort_values(ascending=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TD66fT3iqa8"
   },
   "source": [
    "3. Visualize total cost per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(y=df2.index, x=df2.TotalPrice.sort_values(ascending=False));\n",
    "#w/out United Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYvgBulfcqpu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(y=df2.iloc[1:].index, x=df2.iloc[1:].TotalPrice.sort_values(ascending=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwfJuBVCQGWR"
   },
   "source": [
    "#### The UK not only has the most sales revenue, but also the most customers. Since the majority of this data set contains orders from the UK, we can explore the UK market further by finding out what products the customers buy together and any other buying behaviors to improve our sales and targeting strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A25nnqIQGWR"
   },
   "source": [
    "### vii. Explore the UK Market\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtWchB1Ziqa9"
   },
   "source": [
    "1. Create df_uk DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htKw3RmmcsiN"
   },
   "outputs": [],
   "source": [
    "df_uk=df[df.Country=='United Kingdom']\n",
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is5kus2bQGWT"
   },
   "source": [
    "2. What are the most popular products that are bought in the UK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X88j8TqwcuA5"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df_uk.StockCode.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHbqJD7bQGWU"
   },
   "source": [
    "### We will continue analyzing the UK transactions with customer segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAtzWvugQGWV"
   },
   "source": [
    "# 2. RFM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5uUAUQpQGWV"
   },
   "source": [
    "In the age of the internet and e-commerce, companies that do not expand their businesses online or utilize digital tools to reach their customers will run into issues like scalability and a lack of digital precsence. An important marketing strategy e-commerce businesses use for analyzing and predicting customer value is customer segmentation. Customer data is used to sort customers into group based on their behaviors and preferences.\n",
    "\n",
    "**[RFM](https://www.putler.com/rfm-analysis/) (Recency, Frequency, Monetary) Analysis** is a customer segmentation technique for analyzing customer value based on past buying behavior. RFM analysis was first used by the direct mail industry more than four decades ago, yet it is still an effective way to optimize your marketing.\n",
    "<br>\n",
    "<br>\n",
    "Our goal in this Notebook is to cluster the customers in our data set to:\n",
    " - Recognize who are our most valuable customers\n",
    " - Increase revenue\n",
    " - Increase customer retention\n",
    " - Learn more about the trends and behaviors of our customers\n",
    " - Define customers that are at risk\n",
    "\n",
    "We will start with **RFM Analysis** and then compliment our findings with predictive analysis using **K-Means Clustering Algorithms.**\n",
    "\n",
    "- RECENCY (R): Time since last purchase\n",
    "- FREQUENCY (F): Total number of purchases\n",
    "- MONETARY VALUE (M): Total monetary value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Benefits of RFM Analysis\n",
    "\n",
    "- Increased customer retention\n",
    "- Increased response rate\n",
    "- Increased conversion rate\n",
    "- Increased revenue\n",
    "\n",
    "RFM Analysis answers the following questions:\n",
    " - Who are our best customers?\n",
    " - Who has the potential to be converted into more profitable customers?\n",
    " - Which customers do we need to retain?\n",
    " - Which group of customers is most likely to respond to our marketing campaign?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX47_J8OQGWV"
   },
   "source": [
    "### i. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOD15BxHiqa-"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('this will not show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rYM4MJsQGWW"
   },
   "source": [
    "### ii. Review df_uk DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQHyAJNeiqa-"
   },
   "outputs": [],
   "source": [
    "df['InvoiceDate']=pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "df_uk=df[df.Country=='United Kingdom']\n",
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvfCcPLgQGWa"
   },
   "source": [
    "### iii. Recency: Days since last purchase\n",
    "To calculate the recency values, follow these steps in order:\n",
    "\n",
    "1. To calculate recency, we need to choose a date as a point of reference to evaluate how many days ago was the customer's last purchase.\n",
    "2. Create a new column called Date which contains the invoice date without the timestamp\n",
    "3. Group by CustomerID and check the last date of purchase\n",
    "4. Calculate the days since last purchase\n",
    "5. Drop Last_Purchase_Date since we don't need it anymore\n",
    "6. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7Gzn6r6QGWb"
   },
   "source": [
    "1. Choose a date as a point of reference to evaluate how many days ago was the customer's last purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwWODVSZiqa-"
   },
   "outputs": [],
   "source": [
    "#point of reference date\n",
    "\n",
    "ref_date = max(df['InvoiceDate'])\n",
    "ref_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge34PCT0iqa-"
   },
   "source": [
    "2. Create a new column called Date which contains the invoice date without the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsaFUydXiqa_"
   },
   "outputs": [],
   "source": [
    "df_uk['Date']=df_uk['InvoiceDate'].apply(lambda x: x.date())\n",
    "df_uk.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KJzP5Ldiqa_"
   },
   "source": [
    "3. Group by CustomerID and check the last date of purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8kMt-Uyiqa_"
   },
   "outputs": [],
   "source": [
    "df_uk['Last_Purchase_Date']=df_uk.groupby(['CustomerID'])['Date'].transform(max)\n",
    "df_uk.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Zxs1mzPiqa_"
   },
   "source": [
    "4. Calculate the days since last purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTeKws6giqa_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_uk['Recent_Purchase']=df_uk.groupby('CustomerID')['Last_Purchase_Date'].apply(lambda x:ref_date.date() - x)\n",
    "df_uk['Recent_Purchase']=df_uk.agg({'Recent_Purchase':lambda x:x.astype('timedelta64[D]')})\n",
    "\n",
    "df_uk.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.Recent_Purchase.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAuMSkfsiqa_"
   },
   "source": [
    "5. Drop Last_Purchase_Date since we don't need it anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bNmKqHNiqa_"
   },
   "outputs": [],
   "source": [
    "df_uk=df_uk.drop(['Last_Purchase_Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB9a0AL9iqa_"
   },
   "source": [
    "6. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8e-QiwRiqa_"
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 5))\n",
    "sns.distplot(df_uk.groupby('CustomerID')['Recent_Purchase'].max(), kde=False, bins=80)\n",
    "plt.title('RFM: Recency Value Distribution', fontsize = 15)\n",
    "plt.xlabel('Recent_Purchase')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAjKZD0KQGWg"
   },
   "source": [
    "### iv. Frequency: Number of purchases\n",
    "\n",
    "To calculate how many times a customer purchased something, we need to count how many invoices each customer has. To calculate the frequency values, follow these steps in order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDoNslseiqbA"
   },
   "source": [
    "1. Make a copy of df_uk and drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gk2gokFiqbA"
   },
   "outputs": [],
   "source": [
    "df_uk=df_uk.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KrnuXrLiqbA"
   },
   "source": [
    "2. Calculate the frequency of purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LTM_cxpiqbA"
   },
   "outputs": [],
   "source": [
    "df_uk['Frequency'] = df_uk.groupby('CustomerID').InvoiceNo.transform('nunique')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9NNBCNgiqbA"
   },
   "source": [
    "3. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUtZAHu1iqbA"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.distplot(df_uk.groupby ('CustomerID')['Frequency'].max(),kde=False, bins=200)\n",
    "plt.title('RFM: Frequency Value Distributions', fontsize=15)\n",
    "plt.xlim(-10, 60)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUY3gKjQQGWh"
   },
   "source": [
    "### v. Monetary: Total amount of money spent\n",
    "\n",
    "The monetary value is calculated by adding together the cost of the customers' purchases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_4_QLWtiqbA"
   },
   "source": [
    "1. Calculate sum total cost by customers and named \"Monetary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bilKBqvIiqbB"
   },
   "outputs": [],
   "source": [
    "df_uk['Monetary'] = df_uk.groupby('CustomerID').TotalPrice.transform('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYsaCPRDiqbB"
   },
   "source": [
    "2. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd41fD67iqbB"
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,5))\n",
    "sns.distplot(df_uk.groupby('CustomerID')['Monetary'].max(), kde=False, bins=400)\n",
    "plt.title('RFM: Monetary Value Distribution', fontsize=15)\n",
    "plt.xlim(-10000, 40000)\n",
    "plt.xlabel('Monetary')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeaecPkSQGWj"
   },
   "source": [
    "### vi. Create RFM Table\n",
    "Merge the recency, frequency and motetary dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.rename(columns = {'Recent_Purchase':'Recency'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M88KNSbyiqbB"
   },
   "outputs": [],
   "source": [
    "df_rfm = df_uk[['Recency', 'Frequency', 'Monetary']].drop_duplicates().rename(index=df_uk['CustomerID'])\n",
    "df_rfm.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULWwsxCkQGWl"
   },
   "source": [
    "# 3. Customer Segmentation with RFM Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZrxUBX4iqbB"
   },
   "source": [
    "Businesses have this ever-lasting urge to understand their customers. The better you understand the customer, the better you serve them, and the higher the financial gain you receive from that customer. Since the dawn of trade, this process of understanding customers for a strategic gain has been there practiced and this task is known majorly as [Customer Segmentation](https://clevertap.com/blog/rfm-analysis/).\n",
    "Well as the name suggests, Customer Segmentation could segment customers according to their precise needs. Some of the common ways of segmenting customers are based on their Recency-Frequency-Monatory values, their demographics like gender, region, country, etc, and some of their business-crafted scores. You will use Recency-Frequency-Monatory values for this case.\n",
    "\n",
    "In this section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anOsOGpfQGWl"
   },
   "source": [
    "## Calculate RFM Scoring\n",
    "\n",
    "The simplest way to create customer segments from an RFM model is by using **Quartiles**. We will assign a score from 1 to 4 to each category (Recency, Frequency, and Monetary) with 4 being the highest/best value. The final RFM score is calculated by combining all RFM values. For Customer Segmentation, you will use the df_rfm data set resulting from the RFM analysis.\n",
    "<br>\n",
    "<br>\n",
    "**Note**: Data can be assigned into more groups for better granularity, but we will use 4 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiwXSsP7iqbB"
   },
   "source": [
    "1. Divide the df_rfm into quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFQJGPYHiqbC"
   },
   "outputs": [],
   "source": [
    "quantiles = df_rfm.quantile(q=[0.25, 0.50, 0.75])\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnkzCAf9QGWo"
   },
   "source": [
    "### i. Creating the RFM Segmentation Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLIB-z-_iqbC"
   },
   "source": [
    "1. Create two functions, one for Recency and one for Frequency and Monetary. For Recency, customers in the first quarter should be scored as 4, this represents the highest Recency value. Conversely, for Frequency and Monetary, customers in the last quarter should be scored as 4, representing the highest Frequency and Monetary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXnW03R8iqbC"
   },
   "outputs": [],
   "source": [
    "# Arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)\n",
    "def Recency_func(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 4\n",
    "    elif x<= d[p][0.50]:\n",
    "        return 3\n",
    "    elif x<= d[p][0.75]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)\n",
    "def FM_func(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 1\n",
    "    elif x<= d[p][0.50]:\n",
    "        return 2\n",
    "    elif x<= d[p][0.75]:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLDK_XeLiqbC"
   },
   "source": [
    "2. Score customers from 1 to 4 by applying the functions you have created. Also create separate score column for each value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plReZMcQiqbC"
   },
   "outputs": [],
   "source": [
    "df_rfm['R_Quartile']=df_rfm['Recency'].apply(Recency_func, args=('Recency', quantiles))\n",
    "df_rfm['F_Quartile']=df_rfm['Frequency'].apply(FM_func, args=('Frequency', quantiles))\n",
    "df_rfm['M_Quartile']=df_rfm['Monetary'].apply(FM_func, args=('Monetary', quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JskteCFdQGWq"
   },
   "source": [
    "3. Now that scored each customer, you'll combine the scores for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYfoHF6QiqbC"
   },
   "outputs": [],
   "source": [
    "df_rfm['RFM_Scores']=df_rfm.R_Quartile.apply(str) + df_rfm.F_Quartile.apply(str) + df_rfm.M_Quartile.apply(str)\n",
    "df_rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWwWeyjPiqbC"
   },
   "source": [
    "4. Define rfm_level function that tags customers by using RFM_Scrores and Create a new variable RFM_Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxXk7jFPiqbD"
   },
   "outputs": [],
   "source": [
    "df_rfm['RFM_Points'] = df_rfm[['R_Quartile', 'F_Quartile', 'M_Quartile']].sum(axis=1).astype('float')\n",
    "df_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(np.zeros(len(df_rfm)))\n",
    "\n",
    "for i in range(len(df_rfm)):\n",
    "    if df_rfm['RFM_Points'].iloc[i] ==12: label[i] = \"Best Customers\"  \n",
    "    elif df_rfm['RFM_Points'].iloc[i] ==11: label[i] = \"Loyal Customers\"\n",
    "    elif df_rfm['RFM_Points'].iloc[i] >= 9 : label[i] = \"Big Spenders\"\n",
    "    elif df_rfm['RFM_Points'].iloc[i] >= 7 : label[i] = \"About to Lose Customers\"\n",
    "    elif df_rfm['RFM_Points'].iloc[i] >= 5 : label[i] = \"Lost Customers\"\n",
    "    else : label[i] = \"Forever Lost Customers\"       \n",
    "        \n",
    "df_rfm['RFM_Points_Segments'] = label\n",
    "df_rfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.groupby('RFM_Points').agg({'Recency': ['mean', 'median', 'min', 'max'],\n",
    "                                 'Frequency': ['mean', 'median', 'min', 'max'],\n",
    "                                 'Monetary': ['mean', 'median', 'min', 'max', 'count']}).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq36PiX3iqbD"
   },
   "source": [
    "5. Calculate average values for each RFM_Level, and return a size of each segment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn5r5P2WiqbD"
   },
   "outputs": [],
   "source": [
    "avg_RFM_Points = df_rfm.groupby('RFM_Points_Segments').RFM_Points.mean().apply(lambda x:round(x,1))\n",
    "size_RFM_Points = df_rfm['RFM_Points_Segments'].value_counts()\n",
    "summary = pd.concat([avg_RFM_Points, size_RFM_Points], axis=1).rename(columns={'RFM_Points': 'avg_RFM_Points',\n",
    "                                                                              'RFM_Points_Segments': 'size_RFM_Points'}).sort_values('avg_RFM_Points', ascending=False)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.groupby('RFM_Points_Segments').agg({'Recency': ['mean', 'median', 'min', 'max'],\n",
    "                                          'Frequency': ['mean', 'median', 'min', 'max'],\n",
    "                                          'Monetary': ['mean', 'median', 'min', 'max'],\n",
    "                                          'RFM_Points': ['mean', 'median', 'min', 'max', 'count'],}).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuZ5Olo4iqbD"
   },
   "source": [
    "## Plot RFM Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STfELckwiqbD"
   },
   "source": [
    "1. Create your plot and resize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oef37q3diqbD"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "summary.size_RFM_Points.plot(ax=ax,color='b',label='Size',kind='bar',width=0.3)\n",
    "plt.legend(bbox_to_anchor=(0.0, 0.90), loc=2, borderaxespad=0.)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "summary.avg_RFM_Points.plot(ax=ax2,color='r',label='Average', marker='o')\n",
    "plt.legend(bbox_to_anchor=(0.0, 0.80), loc=2, borderaxespad=0.)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhOe2bb6QGWu"
   },
   "source": [
    "Using customer segmentation categories found [here](http://www.blastam.com/blog/rfm-analysis-boosts-sales) we can formulate different marketing strategies and approaches for customer engagement for each type of customer.\n",
    "\n",
    "Note: The author in the article scores 1 as the highest and 4 as the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te_6gUR5iqbD"
   },
   "source": [
    "2. How many customers do we have in each segment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gs4rP-0viqbD"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.pie(df_rfm['RFM_Points_Segments'].value_counts().sort_index(),autopct='%1.1f%%',shadow=False,startangle=0)\n",
    "plt.legend(df_rfm['RFM_Points_Segments'].value_counts().sort_index().index,bbox_to_anchor=(1.45,0.5),loc='center right')\n",
    "plt.title('Customer Segmentation Distribution')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "df_rfm=df_rfm.sort_values('RFM_Points_Segments')\n",
    "sns.pairplot(df_rfm[['Recency', 'Frequency', 'Monetary', 'RFM_Points_Segments']], hue='RFM_Points_Segments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the graph\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.pairplot(df_rfm[['R_Quartile', 'F_Quartile', 'M_Quartile', 'RFM_Points_Segments']], hue='RFM_Points_Segments', plot_kws={'s':200});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(df_rfm['RFM_Points_Segments'], df_rfm['Recency'])\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(df_rfm['RFM_Points_Segments'], df_rfm['Frequency'])\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(df_rfm['RFM_Points_Segments'], df_rfm['Monetary'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(df_rfm['RFM_Points_Segments'], df_rfm['Recency'])\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(df_rfm['RFM_Points_Segments'], df_rfm['Frequency'])\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(df_rfm['RFM_Points_Segments'], df_rfm['Monetary'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.stripplot(df_rfm['RFM_Points_Segments'], df_rfm['Recency'])\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.stripplot(df_rfm['RFM_Points_Segments'], df_rfm['Frequency'])\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.stripplot(df_rfm['RFM_Points_Segments'], df_rfm['Monetary'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "sns.distplot(df_rfm.Recency,bins=100)\n",
    "plt.title('Recency Distribution')\n",
    "plt.xlabel('Recency')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.distplot(df_rfm['Frequency'],color='red',bins=150)\n",
    "plt.title('Frequency Distribution')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('')\n",
    "plt.xlim(-10, 50)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.distplot(df_rfm['Monetary'],color='green',bins=300)\n",
    "plt.title('Monetary Distribution')\n",
    "plt.xlabel('Monetary')\n",
    "plt.ylabel('')\n",
    "plt.xlim(-5000, 10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "sns.kdeplot(x='Recency',data=df_rfm,hue=\"RFM_Points_Segments\",shade=True)\n",
    "plt.title('Recency Distribution')\n",
    "plt.xlabel('Recency')\n",
    "plt.ylabel('Count')\n",
    "plt.ylim(0, 0.008)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.kdeplot(x='Frequency',data=df_rfm,hue=\"RFM_Points_Segments\",shade=True)\n",
    "plt.title('Frequency Distribution')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('')\n",
    "plt.xlim(-1, 10)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.kdeplot(x='Monetary',data=df_rfm,hue=\"RFM_Points_Segments\",shade=True)\n",
    "plt.title('Monetary Distribution')\n",
    "plt.xlabel('Monetary')\n",
    "plt.ylabel('')\n",
    "plt.xlim(-5000, 5000)\n",
    "plt.ylim(0, 0.0005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.title('R_Quartile Distribution')\n",
    "sns.countplot(x='R_Quartile', hue='RFM_Points_Segments', data=df_rfm)\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.title('F_Quartile Distribution')\n",
    "sns.countplot(x='F_Quartile',  hue='RFM_Points_Segments', data=df_rfm)\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.title('M_Quartile Distribution')\n",
    "sns.countplot(x='M_Quartile',  hue='RFM_Points_Segments', data=df_rfm)\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num of customers in each segment:\n",
    "df_rfm.RFM_Points_Segments.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RwemvLyQGWv"
   },
   "source": [
    "### 3. Applying K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6WZ0PnZQGWv"
   },
   "source": [
    "Now that we have our customers segmented into 6 different categories, we can gain further insight into customer behavior by using predictive models in conjuction with out RFM model.\n",
    "Possible algorithms include **Logistic Regression**, **K-means Clustering**, and **K-nearest Neighbor**. We will go with [K-Means](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) since we already have our distinct groups determined. K-means has also been widely used for market segmentation and has the advantage of being simple to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrWIRLkMiqbE"
   },
   "source": [
    "## Data Pre-Processing and Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLoRGR6NiqbE"
   },
   "outputs": [],
   "source": [
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk['InvoicePeriod']=pd.to_datetime(df_uk['InvoiceDate']).apply(lambda x: x.to_period('M'))\n",
    "df_line=pd.DataFrame(df_uk.groupby('InvoicePeriod').CustomerID.nunique())\n",
    "df_line['TotalPrice']=df_uk.groupby('InvoicePeriod').TotalPrice.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_line.CustomerID.plot(ax=ax,color='b',label='Unique Customers', marker='o')\n",
    "plt.legend(bbox_to_anchor=(0.0, 0.90), loc=2, borderaxespad=0.)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "df_line.TotalPrice.plot(ax=ax2,color='r',label='TotalPrice',linestyle='--', marker='o')\n",
    "plt.legend(bbox_to_anchor=(0.0, 0.80), loc=2, borderaxespad=0.)\n",
    "plt.gcf().axes[1].yaxis.get_major_formatter().set_scientific(False) # remove scientific notation\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_span=pd.DataFrame(df.groupby('CustomerID').InvoiceDate.apply(lambda x:max(x).date() - min(x).date()))\n",
    "life_span=life_span.agg({'InvoiceDate':lambda x:x.astype('timedelta64[D]')}).rename(columns= {'InvoiceDate':'life_span'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans=df_rfm[['Recency', 'Frequency', 'Monetary']]\n",
    "df_kmeans['life_span']=life_span\n",
    "df_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6SGV0xoQGWw"
   },
   "source": [
    "### i. Define and Plot Feature Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpMAiWNBiqbE"
   },
   "source": [
    "Create Heatmap and evaluate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6Cv8_EqiqbE"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(df_kmeans.corr(),annot=True, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WhL5MBEQGWy"
   },
   "source": [
    "### ii. Visualize Feature Distributions\n",
    "\n",
    "To get a better understanding of the dataset, you can costruct a scatter matrix of each of the three features in the RFM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHE0Vb0KiqbE"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.pairplot(df_kmeans[['Recency', 'Frequency', 'Monetary', 'life_span']], hue='life_span');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.pairplot(df_kmeans[['Recency', 'Frequency', 'Monetary', 'life_span']], hue='life_span', plot_kws={'s':200});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2RsjzjbQGWz"
   },
   "source": [
    "### iii. Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXsrPpegiqbF"
   },
   "source": [
    "1. You can use the logarithm method to normalize the values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-cBU8jxiqbF"
   },
   "outputs": [],
   "source": [
    "def col_plot(df,col_name,iqr=1.5):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    plt.subplot(141) \n",
    "    plt.hist(df[col_name], bins = 20)\n",
    "    f=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\n",
    "    \n",
    "    plt.axvline(x=df[col_name].mean() + 3*df[col_name].std(),color='red')\n",
    "    plt.axvline(x=df[col_name].mean() - 3*df[col_name].std(),color='red')\n",
    "    plt.xlabel(col_name)\n",
    "    plt.tight_layout\n",
    "    plt.xlabel(\"Histogram ±3z\")\n",
    "    plt.ylabel(col_name)\n",
    "\n",
    "    plt.subplot(142)\n",
    "    plt.boxplot(df[col_name], whis = iqr)\n",
    "    plt.xlabel(f\"IQR={iqr}\")\n",
    "\n",
    "    plt.subplot(143)\n",
    "    plt.boxplot(df[col_name].apply(f), whis = iqr)\n",
    "    plt.xlabel(f\"ROOT SQUARE - IQR={iqr}\")\n",
    "\n",
    "    plt.subplot(144)\n",
    "    plt.boxplot(np.log(df[col_name]+1), whis = iqr)\n",
    "    plt.xlabel(f\"LOGARITMIC - IQR={iqr}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_kmeans:\n",
    "    col_plot(df_kmeans, i, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['Recency', \n",
    "          'Frequency', \n",
    "          'Monetary', \n",
    "          'life_span',\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log=df_kmeans.copy()\n",
    "for i in features:\n",
    "    df_log[i]=np.log(df_log[i]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df:pd.DataFrame, col_name:str, p=1.5) ->int:\n",
    "    ''' \n",
    "    this function detects outliers based on 3 time IQR and\n",
    "    returns the number of lower and uper limit and number of outliers respectively\n",
    "    '''\n",
    "    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                      \n",
    "    upper_limit = third_quartile+(p*IQR)\n",
    "    lower_limit = first_quartile-(p*IQR)\n",
    "    outlier_count = 0\n",
    "                      \n",
    "    for value in df[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "    return lower_limit, upper_limit, outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr=2\n",
    "print(f\"Number of Outliers for {iqr}*IQR after Logarithmed\\n\")\n",
    "\n",
    "total=0\n",
    "for col in features:\n",
    "    if detect_outliers(df_log, col)[2] > 0:\n",
    "        outliers=detect_outliers(df_log, col, iqr)[2]\n",
    "        total+=outliers\n",
    "        print(\"{} outliers in '{}'\".format(outliers,col))\n",
    "print(\"\\n{} OUTLIERS TOTALLY\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr=2\n",
    "for i in ['Frequency','Monetary']:\n",
    "    lower,upper,_=detect_outliers(df_log,i,iqr)\n",
    "    df_log=df_log[(df_log[i]>lower)&(df_log[i]<upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC6dHnqKiqbF"
   },
   "source": [
    "2. Plot normalized data with scatter matrix or pairplot. Also evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB9jDUPriqbF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_scaled = pd.DataFrame(StandardScaler().fit_transform(df_log),\n",
    "                         columns=df_kmeans.columns,index=df_log.index)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.pairplot(df_scaled[['Recency', 'Frequency', 'Monetary','life_span']]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35c0aDixQGW4"
   },
   "source": [
    "## K-Means Implementation\n",
    "\n",
    "For k-means, you have to set k to the number of clusters you want, but figuring out how many clusters is not obvious from the beginning. We will try different cluster numbers and check their [silhouette coefficient](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html). The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). \n",
    "<br>\n",
    "<br>\n",
    "**Note**: K-means is sensitive to initializations because they are critical to qualifty of optima found. Thus, we will use smart initialization called \"Elbow Method\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JloMSEeriqbF"
   },
   "source": [
    "### i. Define the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McFq6IWZt5hg"
   },
   "source": [
    "[The Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyclustertend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from pyclustertend import hopkins\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopkins(df_scaled,df_scaled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = []\n",
    "K = range(2,10)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters = k).fit((df_scaled))\n",
    "    ssd.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(K, ssd, \"bo-\")\n",
    "plt.xlabel(\"Different k values\")\n",
    "plt.ylabel(\"inertia-error\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "kmeans = KMeans()\n",
    "visu = KElbowVisualizer(kmeans, k = (2,10))\n",
    "visu.fit(df_scaled)\n",
    "visu.poof();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACT_d0UpwUSC"
   },
   "source": [
    "[Silhouette Coefficient](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS4TLbRniqbG"
   },
   "outputs": [],
   "source": [
    "ssd =[]\n",
    "\n",
    "K = range(2,10)\n",
    "\n",
    "for k in K:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(df_scaled)\n",
    "    ssd.append(model.inertia_)\n",
    "    print(f'Silhouette Score for {k} clusters: {silhouette_score(df_scaled, model.labels_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6dW2MWZiqbG"
   },
   "source": [
    "### ii. Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXM5ksoPiqbG"
   },
   "source": [
    "Fit the K-Means Algorithm with the optimal number of clusters you decided and save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geMuViLniqbG"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 4).fit(df_scaled)\n",
    "labels = kmeans.labels_\n",
    "df_scaled['Kmeans_Label_ID']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=df_scaled.groupby('Kmeans_Label_ID').Frequency.mean().sort_values().index\n",
    "values=['Bronze','Silver','Gold','Diamond']\n",
    "dictionary = dict(zip(keys, values))\n",
    "\n",
    "df_scaled['Kmeans_Label']=df_scaled.Kmeans_Label_ID.apply(lambda x:dictionary[x] )\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqcSwNZTQGW7"
   },
   "source": [
    "### iii. Visualize the Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfx5kzPriqbG"
   },
   "source": [
    "1. Create a scatter plot and select cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyrovJB0iqbH"
   },
   "outputs": [],
   "source": [
    "df_scaled.Kmeans_Label.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.Kmeans_Label.value_counts().plot.bar(width=0.3)\n",
    "plt.title('Distribution of Clustering Based on K-Means');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(14,6)) # sharey=True ile y eksen labels lari ortak kullanirlar.\n",
    "ax1.set_title('Recency-Frequency')\n",
    "ax1.set_xlabel('Recency')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.scatter(df_scaled.iloc[:,0],df_scaled.iloc[:,1],c=kmeans.labels_,cmap=\"rainbow\")\n",
    "ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300,alpha=0.9, label = 'Centroids')\n",
    "\n",
    "ax2.set_title(\"Frequency-Monetary\")\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_ylabel('Monetary')\n",
    "ax2.scatter(df_scaled.iloc[:,1],df_scaled.iloc[:,2],c=kmeans.labels_,cmap=\"rainbow\")\n",
    "ax2.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2], s=300,alpha=0.9, label = 'Centroids');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.pairplot(df_scaled[['Recency', 'Frequency', 'Monetary','life_span','Kmeans_Label']],hue='Kmeans_Label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.stripplot(df_scaled['Kmeans_Label'], df_scaled['Recency'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.stripplot(df_scaled['Kmeans_Label'], df_scaled['Frequency'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.stripplot(df_scaled['Kmeans_Label'], df_scaled['Monetary'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.stripplot(df_scaled['Kmeans_Label'], df_scaled['life_span'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4sHOvlniqbH"
   },
   "source": [
    "2. Visualize Cluster Id vs Recency, Cluster Id vs Frequency and Cluster Id vs Monetary using Box plot. Also evaluate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzPl-LXViqbH"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(df_scaled['Kmeans_Label'], df_scaled['Recency'])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(df_scaled['Kmeans_Label'], df_scaled['Frequency'])\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(df_scaled['Kmeans_Label'], df_scaled['Monetary'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRyku5qJiqbH"
   },
   "source": [
    "### iv. Assign the Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVrkisf9iqbH"
   },
   "outputs": [],
   "source": [
    "RFM_Points_Segments=['Best Customers','Loyal Customers', 'Big Spenders', 'About to Lose Customers', 'Lost Customers', 'Forever Lost Customers']\n",
    "Kmeans_Label=['Diamond','Gold', 'Silver', 'Bronze']\n",
    "pd.crosstab(df_scaled['Kmeans_Label'],df_rfm['RFM_Points_Segments'])[RFM_Points_Segments].loc[Kmeans_Label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DiG6sxS4gWH"
   },
   "source": [
    "**Conclusion Example :**\n",
    "\n",
    "- Cluster 0 : The first cluster belongs to the \"Best Customers\" segment which we saw earlier as they purchase recently (R=4), frequent buyers (F=4), and spent the most (M=4)\n",
    "\n",
    "- Cluster 1 : Second cluster can be interpreted as passer customers as their last purchase is long ago (R<=1),purchased very few (F>=2 & F < 4) and spent little (M>=4 & M < 4).Company has to come up with new strategies to make them permanent members. Low value customers\n",
    "- Cluster 2 : The third cluster is more related to the \"Almost Lost\" segment as they Haven’t purchased for some time(R=1), but used to purchase frequently and spent a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf4OsPmSQGXA"
   },
   "source": [
    "### v. Conclusion\n",
    "\n",
    "Discuss your final results. Compare your own labels from the Customer Segmentation with the labels found by K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFaHgoLoQGXA"
   },
   "source": [
    "How we want to continue this analysis depends on how the business plans to use the results and the level of granularity the business stakeholders want to see in the clusters. We can also ask what range of customer behavior from high to low value customers are the stakeholders interested in exploring. From those answers, various methods of clustering can be used and applied on RFM variable or directly on the transaction data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysCkU1B-iqbI"
   },
   "source": [
    "**Annotation:**\n",
    "\n",
    "Limitations of K-means clustering:\n",
    "\n",
    "1. There is no assurance that it will lead to the global best solution.\n",
    "2. Can't deal with different shapes(not circular) and consider one point's probability of belonging to more than one cluster.\n",
    "\n",
    "These disadvantages of K-means show that for many datasets (especially low-dimensional datasets), it may not perform as well as you might hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiPd_IbnQGVn"
   },
   "source": [
    "# 5. Create Cohort & Conduct Cohort Analysis\n",
    "[Cohort Analysis](https://medium.com/swlh/cohort-analysis-using-python-and-pandas-d2a60f4d0a4d) is specifically useful in analyzing user growth patterns for products. In terms of a product, a cohort can be a group of people with the same sign-up date, the same usage starts month/date, or the same traffic source.\n",
    "Cohort analysis is an analytics method by which these groups can be tracked over time for finding key insights. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n",
    "\n",
    "For e-commerce organizations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. by performing Cohort analysis you can get the following answers to the following questions:\n",
    "\n",
    "- How much effective was a marketing campaign held in a particular time period?\n",
    "- Did the strategy employ to improve the conversion rates of Customers worked?\n",
    "- Should I focus more on retention rather than acquiring new customers?\n",
    "- Are my customer nurturing strategies effective?\n",
    "- Which marketing channels bring me the best results?\n",
    "- Is there a seasonality pattern in Customer behavior?\n",
    "- Along with various performance measures/metrics for your organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhiYivPrQGVo"
   },
   "source": [
    "Since we will be performing Cohort Analysis based on transaction records of customers, the columns we will be dealing with mainly:\n",
    "- Invoice Data\n",
    "- CustomerID\n",
    "- Price\n",
    "- Quantity\n",
    "\n",
    "The following steps will performed to generate the Cohort Chart of Retention Rate:\n",
    "- Month Extraction from InvioceDate column\n",
    "- Assigning Cohort to Each Transaction\n",
    "- Assigning Cohort Index to each transaction\n",
    "- Calculating number of unique customers in each Group of (ChortDate,Index)\n",
    "- Creating Cohort Table for Retention Rate\n",
    "- Creating the Cohort Chart using the Cohort Table\n",
    "\n",
    "The Detailed information about each step is given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo0GB_osiqbI"
   },
   "source": [
    "## Future Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVwPNjpyQGVo"
   },
   "source": [
    "### i. Extract the Month of the Purchase\n",
    "First we will create a function, which takes any date and returns the formatted date with day value as 1st of the same month and Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK1CqlNQiqbI"
   },
   "outputs": [],
   "source": [
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ym(df): \n",
    "    return df.apply(lambda x: x.strftime('%Y-%m')).astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQKsM_9IQGVq"
   },
   "source": [
    "Now we will use the function created above to convert all the invoice dates into respective month date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL17u0dniqbJ"
   },
   "outputs": [],
   "source": [
    "extract_ym(df.InvoiceDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPE7kTz2QGVs"
   },
   "source": [
    "### ii. Calculating time offset in Months i.e. Cohort Index:\n",
    "Calculating time offset for each transaction will allows us to report the metrics for each cohort in a comparable fashion.\n",
    "First, you will create 4 variables that capture the integer value of years, months for Invoice and Cohort Date using the get_date_int() function which you'll create it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_3aYf4FiqbJ"
   },
   "outputs": [],
   "source": [
    "def get_date_int(df, column):\n",
    "    years = df[column].dt.year\n",
    "    months = df[column].dt.month\n",
    "    return years, months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGm1eweDQGVu"
   },
   "source": [
    "You will use this function to extract the integer values for Invoice as well as Cohort Date in 3 seperate series for each of the two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wF_ViD_iqbJ"
   },
   "outputs": [],
   "source": [
    "df['InvoiceMonth'] = df['InvoiceDate'].apply(lambda x: dt(x.year, x.month, 1) )\n",
    "df['cohort_date'] = df.groupby('CustomerID')['InvoiceMonth'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_year, cohort_month = get_date_int(df, 'cohort_date')\n",
    "invoice_year, invoice_month = get_date_int(df, 'InvoiceDate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9jYVljviqbJ"
   },
   "source": [
    "Use the variables created above to calcualte the difference in days and store them in cohort Index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVlAYCbEiqbJ"
   },
   "outputs": [],
   "source": [
    "years_diff = invoice_year - cohort_year\n",
    "months_diff = invoice_month - cohort_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-t76CXSQGVw"
   },
   "source": [
    "## Create 1st Cohort: User number & Retention Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKvUWci9iqbJ"
   },
   "source": [
    "### i. Pivot Cohort and Cohort Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-8HzlZWiqbK"
   },
   "outputs": [],
   "source": [
    "df['CohortIndex'] = years_diff * 12 + months_diff + 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_count = df.groupby(['cohort_date', 'CohortIndex'])\n",
    "cohort_data = grouping_count['CustomerID'].apply(pd.Series.nunique)\n",
    "cohort_data = cohort_data.reset_index()\n",
    "cohort_counts = cohort_data.pivot(index='cohort_date',\n",
    "                                  columns='CohortIndex',\n",
    "                                  values='CustomerID')\n",
    "cohort_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_sizes = cohort_counts.iloc[:,0]\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0).apply(lambda x: round(x,2))\n",
    "retention.index = retention.index.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention.round(3) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63TIyBY6iqbK"
   },
   "source": [
    "### ii. Visualize analysis of cohort 1 using seaborn and matplotlib modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention.T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY7mPvCAiqbK"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.title('Retention rates')\n",
    "sns.heatmap(data = retention, annot = True, fmt = '.0%',vmin = 0.0,vmax = 0.5,cmap = 'BuGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yORYolvqQGV0"
   },
   "source": [
    "## Create the 2nd Cohort: Average Quantity Sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu1hM3CFiqbK"
   },
   "source": [
    "### i. Pivot Cohort and Cohort Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ8jlhPEiqbK"
   },
   "outputs": [],
   "source": [
    "grouping_qty = df.groupby(['cohort_date', 'CohortIndex'])\n",
    "cohort_data_qty = grouping_qty['Quantity'].mean()\n",
    "cohort_data_qty = cohort_data_qty.reset_index()\n",
    "average_quantity = cohort_data_qty.pivot(index='cohort_date',\n",
    "                                     columns='CohortIndex',\n",
    "                                     values='Quantity')\n",
    "average_quantity.index = average_quantity.index.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3PJHMS6iqbK"
   },
   "source": [
    "### ii. Visualize analysis of cohort 2 using seaborn and matplotlib modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vuHi3wPiqbK"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.title('Average Quantity')\n",
    "sns.heatmap(data = average_quantity, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUoG5yUIQGV3"
   },
   "source": [
    "## Create the 3rd Cohort: Average Sales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKNS-mO5iqbL"
   },
   "source": [
    "### i. Pivot Cohort and Cohort Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2s-zyWeiqbL"
   },
   "outputs": [],
   "source": [
    "grouping_price = df.groupby(['cohort_date', 'CohortIndex'])\n",
    "cohort_data_price = grouping_price['TotalPrice'].mean()\n",
    "cohort_data_price = cohort_data_price.reset_index()\n",
    "average_price = cohort_data_price.pivot(index='cohort_date',\n",
    "                                     columns='CohortIndex',\n",
    "                                     values='TotalPrice')\n",
    "average_price.index = average_price.index.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRGOpeUPiqbL"
   },
   "source": [
    "### ii. Visualize analysis of cohort 3 using seaborn and matplotlib modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYkkDncXiqbL"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.title('Average Price')\n",
    "sns.heatmap(data = average_price, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD9lu1ExQGV5"
   },
   "source": [
    "For e-commerce organisations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. by performing Cohort analysis you can get answers to following questions:\n",
    "\n",
    "- How much effective was a marketing campaign held in a particular time period?\n",
    "- Did the strategy employed to improve the conversion rates of Customers worked?\n",
    "- Should I focus more on retention rather than acquiring new customers?\n",
    "- Are my customer nurturing strategies effective?\n",
    "- Which marketing channels bring me the best results?\n",
    "- Is there a seasoanlity pattern in Customer behahiour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX_Y6S36iqbL"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lf4OsPmSQGXA"
   ],
   "name": "Customer_Segmentation_Student_Notebook_Capstone_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
